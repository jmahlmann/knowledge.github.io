{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Computational Plasma Physics at Dartmouth College knowledge base! Here, you find essential materials, tutorials, and guidelines to help you begin your research in our plasma and space physics groups. Whether you're a new graduate student, undergraduate researcher, or visiting scholar, this collection of resources will help you navigate the fundamental tools and practices we use in computational analysis of plasma phenomena. We encourage you to explore these materials and reach out to our community members with any questions.</p> <p></p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#tag:beginner","title":"Beginner","text":"<ul> <li>            Discovery Cluster          </li> <li>            HPC Basics          </li> </ul>"},{"location":"tags/#tag:dartmouth-infrastructure","title":"Dartmouth Infrastructure","text":"<ul> <li>            Discovery Cluster          </li> </ul>"},{"location":"tags/#tag:research-tools","title":"Research Tools","text":"<ul> <li>            HPC Basics          </li> <li>            entity          </li> <li>            jupyter notebooks          </li> </ul>"},{"location":"hpc/discovery-getting-started/","title":"Discovery Cluster","text":"<p>Discovery is Dartmouth's high-performance computing cluster that provides substantial computational resources for research. The system features over 6,000 computational cores running CentOS Linux and is equipped with powerful GPU capabilities, including 10 A100, 72 A5500, and 12 V100 GPUs. For data storage, Discovery connects to a 2.6 PB DartFS system. </p> <p>With Dartmouth affiliation, you can request your Discovery account. The Dartmouth Research Computing Bill of Rights provides the research community with a foundation of computational resources.</p>","tags":["Dartmouth Infrastructure","Beginner"]},{"location":"hpc/discovery-getting-started/#connecting-to-the-discovery-cluster","title":"Connecting to the Discovery Cluster","text":"<p>After receiving your research computing access, you can connect to the cluster using Dartmouth VPN and the Secure Shell (SSH) protocol. In a terminal window, type:</p> <pre><code>ssh netID@discovery.dartmouth.edu\n</code></pre> <p>Usually, your Dartmouth netID is your username (replace above). You will be prompted to enter your password and authentication, be sure to follow safe password best practices. See also our resources on HPC basics.</p>","tags":["Dartmouth Infrastructure","Beginner"]},{"location":"hpc/discovery-getting-started/#dartmouth-physics-astronomy-on-discovery","title":"Dartmouth Physics &amp; Astronomy on Discovery","text":"<p>Dartmouth provides researchers with free access to a subvented HPC computing tier. This service offers immediate access to computing resources for computationally intensive tasks. Users receive up to 32 CPU cores, 256GB shared memory, and full access to large-memory HPC systems. Storage options include a 50GB home directory and up to 5TB scratch space. The service also includes comprehensive consulting and support for HPC management, data science, and scientific programming. Research groups may have customized allocations and dedicated lab spaces.</p> EPaCO <p>Extreme Plasmas around Compact Objects (EPaCO) group led by Prof. Mahlmann. Dedicated lab storage space for EPaCO can be accessed via:</p> <pre><code>/dartfs/rc/lab/E/EPaCO\n</code></pre> <p>Requesting access to lab ressources: E-mail to jens.f.mahlmann@dartmouth.edu.</p>","tags":["Dartmouth Infrastructure","Beginner"]},{"location":"hpc/hpc-usage-basics/","title":"HPC Basics","text":"<p>Here, you will learn essential skills for working with performance-optimized computing resources. We will guide you through making your first SSH connection to a remote cluster, navigating file systems, and optimizing your workflow for maximum efficiency. Do not worry if it seems overwhelming at first; we will break down each concept into manageable steps and provide hands-on examples to help you build routines. </p>","tags":["Research Tools","Beginner"]},{"location":"hpc/hpc-usage-basics/#basic-shell-commands","title":"Basic Shell Commands","text":"<p>In this section, we'll explore essential command-line interface (CLI) commands that serve as the building blocks for navigating and manipulating your system through the terminal.</p> Command Description Example <code>cd</code> Change into another directory <code>cd -</code> <code>~</code> Path of the user's home directory <code>cd ~</code> <code>ls</code> List all files and directories in current path <code>ls</code> <code>pwd</code> Displays the current path <code>pwd</code> <code>mkdir</code> Create a new directory <code>mkdir new_directory</code> <code>rm</code> Remove files or directories <code>rm file</code> <code>rm -rf</code> Force remove files or directories (handle with care) <code>rm -rf file</code> <code>cp</code> Copy files <code>cp file1 file2</code> <code>mv</code> Move files (can be used to rename files) <code>mv file1 directory/</code> <code>du</code> Show directory contents and storage size <code>du -h</code> <code>ln -s</code> Create a symbolic link <code>ln -s /path/to/source /path/to/link</code> <p>Take some time to familiarize yourself with these basic CLI commands and try them in practice. They will help you efficiently navigate and work within the cluster environment.</p> <p>Dartmouth Discovery Cluster</p> <p>To familiarize yourself with these basic commands, navigate to your group's lab space on Dartmouth's discovery cluster. Create a symbolic link to the lab space in your home directory.</p>","tags":["Research Tools","Beginner"]},{"location":"hpc/hpc-usage-basics/#secure-shell-ssh-protocol","title":"Secure Shell (SSH) Protocol","text":"<p>SSH (Secure Shell) is a network protocol that provides a secure way to access and manage remote computers by establishing an encrypted connection between client and server. To set it up on your local machine, you can go through the following steps (see also other online resources).</p> <ol> <li> <p>Setting up an SSH key locally. In a terminal window on your local machine, run the following command to set up an shh key:</p> <p><pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre> This generates a new SSH key using the provided email address as a label. When prompted to \"Enter a file in which to save the key,\" press Enter to accept the default file location. You can choose a passphrase to secure your SSH keys.</p> </li> <li> <p>Using SSH keys to access servers without password. In a terminal window on your local machine, run the following command to copy your SSH key to a remote host:</p> <pre><code>ssh-copy-id -i ~/.ssh/mykey user@host\n</code></pre> <p>You will be prompted to enter the remote host's access data. After the succesfull ssh-key communication, future logins should not require additional password requests. However, systems may have special security requirements.</p> <p>Dartmouth Discovery Cluster</p> <p>Dartmouth infrastructure has some caveats when using ssh-keys for remote connections.</p> </li> <li> <p>Create aliases for remote hosts. It can be a substantial shortcut to abbreviate remote host names and addresses. Open a terminal window in your home directory and access the following file:</p> <pre><code>.ssh/config\n</code></pre> <p>In this file, you can set up aliases for remote hosts. For basic functionality, use:</p> <pre><code>Host alias_name\n    HostName host_address\n    User user_name\n</code></pre> <p>You can now access the remote host by using the alias </p> <pre><code>ssh alias_name\n</code></pre> <p>Dartmouth Discovery Cluster</p> <p>For Dartmouth infrastructure, the alias could read: <pre><code>Host discovery\n    HostName discovery.dartmouth.edu\n    User netID\n</code></pre></p> </li> </ol>","tags":["Research Tools","Beginner"]},{"location":"hpc/hpc-usage-basics/#transfering-files-with-rsync","title":"Transfering files with rsync","text":"<p>Rsync (remote synchronization) is a powerful command-line utility for efficiently transferring and synchronizing files between different locations, whether they are on your local machine or across remote systems. This tool is particularly valuable because it uses a delta-transfer algorithm that only copies the differences between source and destination files, significantly reducing the amount of data that needs to be transferred. We will explore how to use rsync's basic features and some of its most useful advanced options. The basic syntax is as follows:</p> <pre><code>rsync -arv /path/to/origin/directory path/to/destination/directory\n</code></pre> <p>The option -arv will use archive mode to preserve permissions (a), recursively (r) loop through sub-directories, and provide verbose (v) output. Each path can be a local or remote paths, making rsync an important tool to transfer data between clusters and our local resources. The following option provides the transfer progress for each file:</p> <pre><code>rsync -arv --progress /path/to/origin/directory path/to/destination/directory\n</code></pre>","tags":["Research Tools","Beginner"]},{"location":"software/entity/","title":"entity","text":"<p>Entity is a cutting-edge particle-in-cell (PIC) code that serves as an excellent tool for studying plasma physics in astrophysical environments. What makes Entity particularly special is its coordinate-agnostic design and modern C++17 implementation. Entity offers several advantages: it's highly modular, works efficiently across different computing architectures (both CPUs and GPUs), and comes with a Python library called nt2py for easy data analysis and visualization. The code's flexible design and support for various output formats make it an ideal platform for learning about advanced plasma physics simulations while using industry-standard tools and practices. Check out the entity wiki for all questions at different levels! </p>","tags":["Research Tools"]},{"location":"software/entity/#entity-on-dartmouths-discovery-cluster","title":"Entity on Dartmouth's Discovery Cluster","text":"<p>We provide libraries to fulfill entity's dependencies on Discovery. You can load the required modules by running the following in your Discovery shell:</p> <pre><code>module use --append /dartfs/rc/lab/E/EPaCO/.mods\nmodule load entity/cuda/mpi/a100\n</code></pre> <p>Discovery Cluster</p> <p>If you do not have access to the EPaCO lab space, please check our groups' allocations and user policies.</p> <p>To compile entity, follow the standard procedure and add the following specifications to your cmake command:</p> <pre><code>cmake -B build -D pgen=... -D output=ON -D mpi=ON -D ... CMAKE_CXX_COMPILER=/dartfs-hpc/rc/home/7/f007gj7/epaco/libs/openmpi/bin/mpicxx -D CMAKE_C_COMPILER=/dartfs-hpc/rc/home/7/f007gj7/epaco/libs/openmpi/bin/mpicc\n</code></pre> <p>To run the code, you can again follow standard procedures.</p> <p>Discovery Cluster</p> <p>A typical submit script for Dartmouth infrastructure looks as follows: <pre><code>#!/bin/bash\n\n#SBATCH -J job_name\n#SBATCH --partition gpuq\n#SBATCH --gres=gpu:1\n#SBATCH -o ./entity.out\n#SBATCH -e ./entity.err\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:20:00\n\nmodule use --append /dartfs/rc/lab/E/EPaCO/.mods\nmodule load entity/cuda/mpi/a100\n\nmpirun -np 1 ./entity.xc -input ...\n</code></pre></p>","tags":["Research Tools"]},{"location":"software/jupyter-notebooks/","title":"jupyter notebooks","text":"<p>This guide will explore the process of setting up and using Jupyter notebooks remotely on Dartmouth's Discovery computing cluster, for example to create visualizations for the entity code. Whether you are analyzing simulation data, creating plots, or debugging code, remote Jupyter notebooks provide a flexible and interactive environment for your research needs.</p>","tags":["Research Tools"]},{"location":"software/jupyter-notebooks/#julyter-notebboks-on-dartmouths-discovery-cluster","title":"Julyter Notebboks on Dartmouth's Discovery Cluster","text":"<p>Dartmouth research computing documents how to remotely access Jupyter notebooks. In the following, we will review the key steps. In your Discovery shell, activate conda:</p> <pre><code>source /optnfs/common/miniconda3/etc/profile.d/conda.sh\n</code></pre> <p>First time users should create a new conda environment for using with Jupyter notebooks:</p> <pre><code>conda create --name jupyter python=3.7 anaconda\n</code></pre> <p>Returning users can activate the custom environment by typing </p> <pre><code>conda activate jupyter\n</code></pre> <p>In this environment, you can install custom python packages that will be available for the kernels of remotely accessed notebooks.</p> <p>Research with entity</p> <p>To install the visualization package of the entity toolkit, you can run: <pre><code>pip install nt2py\n</code></pre></p> <p>To launch a Jupyter notebook, execute the following in your Discovery shell: <pre><code>jupyter notebook --no-browser\n</code></pre></p> <p>Once the notebook starts, you will receive information about the IP address and port used to connect to the notebook (e.g., of the form 127.0.0.1:888x), as well as a token that will be used for secured access.</p> <p>In a new shell on your local machine, establish a new connection to the discovery cluster with additional port forwarding (connecting directly to the Jupyter notebook), for example like:</p> <pre><code>ssh -L 888x:127.0.0.1:888x discovery\n</code></pre> <p>Be sure to update the IP address and port number as generated in the output after launching your notebook. Once the connection is established, you can open the Jupyter notebook in your web browser (e.g., Chrome) by navigating to the following web address:</p> <pre><code>http://localhost:888x/\n</code></pre> <p>You can now use a Jupyter notebook running on ths discovery cluster from your web browser. With direct access to all the data stored in your lab space.</p> <p>Dartmouth Discovery Cluster</p> <p>To avoid running computations on login nodes and slowing down the system for other users, launch your Jupyter notbook in an interactive session. For example, you can use the following command to request four CPU cores with asssiated memory: <pre><code>srun --nodes=1 --ntasks-per-node=1 --cpus-per-task=4 --pty /bin/bash\n</code></pre></p>","tags":["Research Tools"]}]}